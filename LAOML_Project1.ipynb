{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c34894b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ac712",
   "metadata": {},
   "source": [
    "**Exercise 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "52c0252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads dataset from specified path. Returns data matrix X and labels y\n",
    "def load_Xy(path):\n",
    "\n",
    "    #Use built-in pandas function to read csv file into a dataframe\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    #Convert the dataframe to a numpy array\n",
    "    data_array = df.to_numpy()\n",
    "\n",
    "    #Separate the features X from the labels y\n",
    "    X = data_array[:,:-1]\n",
    "    y = data_array[:,-1]\n",
    "\n",
    "    #Replace the 0 label with -1\n",
    "    y[y==0] = -1 \n",
    "\n",
    "    #reshape y as a column vector\n",
    "    y = y.reshape(y.shape[0],1)\n",
    "\n",
    "    #return X and y\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "248b9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function and load the data\n",
    "X,y = load_Xy('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "60bf43c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14632"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's count how many malicious apps there are\n",
    "\n",
    "# Filter the y array only where y == -1, then get its dimensions, and then its length\n",
    "y[ y==-1 ].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a07663",
   "metadata": {},
   "source": [
    " There are $14632$ malicious apps in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5b12da99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero entries: 277180\n",
      "Total entries: 2522552\n",
      "Ratio: 0.10988078739308446\n"
     ]
    }
   ],
   "source": [
    "#Count how many non-zero entries are in X\n",
    "non_zero_entries = X[X!=0].shape[0]\n",
    "\n",
    "#Count total elements in X\n",
    "tot_entries = X.shape[0]*X.shape[1]\n",
    "\n",
    "print(\"Non-zero entries: {}\".format(non_zero_entries))\n",
    "print(\"Total entries: {}\".format(tot_entries))\n",
    "print(\"Ratio: {}\".format(non_zero_entries/tot_entries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e843e",
   "metadata": {},
   "source": [
    "Only the $10\\%$ of the elements of the matrix $X$ are non-zero, thus $X$ has a sparsity of $90\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "826da837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns the set of unique elements in the matrix X\n",
    "np.unique(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc0b57",
   "metadata": {},
   "source": [
    "As we can see from the previous line of code, the matrix elements are either 1s or 0s. One hot coding is typically used to translate categorical features into multiple features that can only assume value 1 or 0. However, the features of the data matrix already assume values $\\in \\{0,1\\}$, thus there is no need to apply one-hot coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd438eba",
   "metadata": {},
   "source": [
    "**Exercise 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "47c0207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the data matrix X and the labels y into train and test set, according to ratio r.\n",
    "def train_test_split(X,y,r):\n",
    "\n",
    "    #Check that r is between 0 and 1\n",
    "    if not ( r > 0 and r < 1):\n",
    "        print(\"Error: r must be between 0 and 1\")\n",
    "        return\n",
    "        \n",
    "    #get number of rows\n",
    "    rows = X.shape[0]\n",
    "\n",
    "    #create a indices array for the rows\n",
    "    indices = [i for i in range(rows)]\n",
    "\n",
    "    #shuffle the array randomly\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    #Now that the indices are randomized, we can split in train and test\n",
    "    train_indices = indices[:int(rows*r)]\n",
    "    test_indices = indices[int(rows*r):]\n",
    "\n",
    "    X_train = X[train_indices,:]\n",
    "    X_test = X[test_indices,:]\n",
    "    y_train = y[train_indices]  \n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a05cf253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data matrix into 50% train set and 50% test set\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X,y,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e453d3f3",
   "metadata": {},
   "source": [
    "**Exercise 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5965fdc",
   "metadata": {},
   "source": [
    "Let $X \\in \\R^{m \\times n}$ be the data matrix with data columns $\\textbf{x}_i,\\ldots, \\textbf{x}_n, i = 1,\\ldots, n$, vector $\\textbf{y} \\in \\{-1,1\\}^n$, and a weight vector $\\textbf{w} \\in \\R^m$. If the product $y_i \\textbf{x}_i^\\intercal \\textbf{w} > 0$, the point is correctly classified, and if $y_i \\textbf{x}_i^\\intercal \\textbf{w} <0$, the point is misclassified. We count the points $\\textbf{x}_i^\\intercal \\textbf{w} = 0$ as misclassified, as there is no way of confirming its (mis)classification.\n",
    "\n",
    "We define a function $f$ that assigns value $0$ to misclassified points, and $1$ for correctly classified points. Mathematically, this function would be defined as $f: \\R \\rightarrow \\{0,1\\}$:\n",
    "\n",
    "$f(s) \\coloneqq \\frac{1}{2}(\\text{sign}(s) + 1)$,\n",
    "\n",
    "with $f(0) = 0$.\n",
    "\n",
    "The function for the number of correctly classified points $F: \\R^m \\rightarrow \\Z_{\\geq 0}$ is then defined:\n",
    "\n",
    "$F(\\textbf{w}) = \\sum_{i = 1} ^ n f(y_i\\textbf{x}_i^\\intercal \\textbf{w})$.\n",
    "\n",
    "Since Python does not have a sign function, we write functions that fulfill this task. \n",
    "\n",
    "We generate a random weight vector $w \\in \\R^m$ such that the chance of $w_j = 0$ is approximately zero, but we include a safety measure to stop the function in the case $\\textbf{w}$ contains at least one $0$ element.\n",
    "\n",
    "The functions for the sum of correctly classified points are defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fe2d15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifies the data points. Assigns labels +1 or -1\n",
    "def classify(X,w):\n",
    "\n",
    "    # For each row of X, compute x_i'*w\n",
    "    v = np.dot(X,w)\n",
    "\n",
    "    #if it's > 0, replace with +1\n",
    "    v[v > 0] = 1\n",
    "\n",
    "    #replace with -1 otherwise\n",
    "    v[v <= 0] = -1\n",
    "    return v\n",
    "\n",
    "#Returns number of correctly classified datapoints using weight w\n",
    "def count_correctly_classified(X,y,w):\n",
    "    v = classify(X,w)*y\n",
    "    return len(v[v == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6bf570a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified points: 13448/29332\n"
     ]
    }
   ],
   "source": [
    "w_rand = np.random.rand(X.shape[1],1)*2 - 1\n",
    "\n",
    "print(\"Correctly classified points: {}/{}\".format(count_correctly_classified(X,y,w_rand), y.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fbc6a",
   "metadata": {},
   "source": [
    "The number of correctly classified points for a random weights vector oscillates around $50 \\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cb3c8",
   "metadata": {},
   "source": [
    "**Exercise 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda06b48",
   "metadata": {},
   "source": [
    "$X \\in \\mathbb{R}^{m \\times n}$, $w \\in \\mathbb{R}^{m \\times 1}$, $y \\in \\mathbb{R}^{n \\times 1}$\n",
    "\n",
    "The Hinge-Loss function for logistic regression is:\n",
    "\n",
    "$g(s) = \\log{(1+e^{-s})}$.\n",
    "\n",
    "As such, the cost function for logistic regression is defined as:\n",
    "\n",
    "$J(\\textbf{w}) = \\sum_{i=1}^{n} \\log{(1+e^{-y_i \\textbf{x}_i^T \\textbf{w}})} + \\frac{\\lambda}{2} \\lVert \\textbf{w} \\rVert^2$,\n",
    "\n",
    "with gradient:\n",
    "\n",
    "$\\nabla J (\\textbf{w}) = \\sum_{i=1}^{n} -\\frac {e^{-y_i \\textbf{x}_i^T \\textbf{w}}}{1+e^{-y_i \\textbf{x}_i^T \\textbf{w}}}y_i\\boldsymbol{x}_i + \\lambda \\textbf{w}$.\n",
    "\n",
    "This gradient is determined as follows: consider the functions $g(s) = \\log {(1+e^{-s})}$ and $h_i (\\textbf{w}) = y_i \\textbf{x}i^T \\textbf{w}$. The cost function then becomes:\n",
    "\n",
    "$J(\\textbf{w}) = \\sum_{i=1}^{n} g (h_i (\\textbf{w})) + \\frac{\\lambda}{2}\\lVert\\textbf{w} \\rVert^2$.\n",
    "\n",
    "Note that the derivatives of $g$ and $h_i$ are defined as:\n",
    "\n",
    "$g^\\prime (s) = - \\frac {e^{-s}}  {1+e^{-s}} = - \\frac {1} {1+e^{s}}\\quad$ , $\\quad\\nabla h_i (\\textbf{w}) = y_i \\textbf{x}_i$,\n",
    "\n",
    "and trivially: $\\frac{d}{d\\textbf{w}}\\left(\\frac{\\lambda}{2}\\lVert\\textbf{w} \\rVert^2\\right) = \\lambda \\textbf{w}$.\n",
    "\n",
    "Then, due to the chain rule of the Jacobian, the gradient of the cost function becomes:\n",
    "\n",
    "$\\nabla J (\\textbf{w}) = \\sum_{i=1}^{n} g^\\prime (y_i \\textbf{x}_i^T \\textbf{w}) \\cdot \\nabla h_i (\\textbf{w}) + \\lambda \\textbf{w}$, \n",
    "\n",
    "obtaining the formula above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2eac1c",
   "metadata": {},
   "source": [
    "**Exercise 5 - Normal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9a4200ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of f(s) = log(1 + e^(-s))\n",
    "def fprime(x):\n",
    "    return -1/(1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4d971148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the gradient of J(w) using the formula derived earlier\n",
    "def grad(X,y,w,a,l):\n",
    "    return np.dot( np.transpose(X), y*fprime(y*np.dot(X,w)) ) + l*w\n",
    "\n",
    "# Computes the cost function J(w)\n",
    "def loss(X,y,w,l):\n",
    "    return np.sum( np.log(np.exp(-y*np.dot(X,w)) + 1 ) ) + (np.linalg.norm(w)**2)*0.5*l\n",
    "\n",
    "#Logistic regression function. a (alpha) is step size, l (lambda) is regularization parameter.\n",
    "#N is number of iterations\n",
    "\n",
    "def lr(X,y,a,l,N):\n",
    "\n",
    "    #Generate a random initial weight vector\n",
    "    w = np.random.rand(86,1)\n",
    "\n",
    "    #Perform fixed step gradient descent\n",
    "    for i in range(N):\n",
    "        g = grad(X,y,w,a,l)\n",
    "        w = w - a*g\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c2e8d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ = lr(X_train,y_train,0.00003,15000,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "82d95ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8531978726305741"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = count_correctly_classified(X_test,y_test,w_)/y_test.shape[0]\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a5cda",
   "metadata": {},
   "source": [
    "**Exercise 5 - Sparse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4f939b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses sparse module from scipy library to further speed up computation\n",
    "def grad_sparse(Xs,y,w,l):\n",
    "    return  Xs.transpose()*(y*fprime(y*(Xs*w))) + l*w\n",
    "\n",
    "def lr_sparse(X,y,a,l,N):\n",
    "    #Converts data matrix into sparse format\n",
    "    Xs = csr_matrix(X)\n",
    "\n",
    "    #Generate random initial weight vector\n",
    "    w = np.random.rand(86,1)\n",
    "\n",
    "    for i in range(N):\n",
    "        gradient = grad_sparse(Xs,y,w,l)\n",
    "        w = w - a*gradient\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "abb7da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different version of sparse logistic regression. \n",
    "# The algorithm stops either when\n",
    "#   i) the norm of the gradient is smalled than threshold. This\n",
    "#      indicates that the alogrithm has reached (almost) the minimum\n",
    "#\n",
    "#   ii) the number of iterations exceeds maxit\n",
    "#\n",
    "# Returns the weigth vector w and the number of iterations k\n",
    "def lr_sparse_thr(X,y,a,l,threshold,maxit):\n",
    "    Xs = csr_matrix(X)\n",
    "    w = np.random.rand(86,1)\n",
    "\n",
    "    #Ensures that at least the first iteration is performed\n",
    "    grad_norm = threshold+1\n",
    "    k = 0\n",
    "    while grad_norm > threshold and k < maxit:\n",
    "        g = grad_sparse(Xs,y,w,l)\n",
    "        w = w - a*g\n",
    "        grad_norm  = np.linalg.norm(g)\n",
    "        k += 1\n",
    "\n",
    "    return w,k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f453ce",
   "metadata": {},
   "source": [
    "In this section we search for an optimal value of the regularization parameter $\\lambda$. \n",
    "Sparse linear algebra allows us to perform a large number of iterations in a reasonable time.\n",
    "We can then choose small $\\alpha$, so that the optimal point is reached with precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc5163",
   "metadata": {},
   "source": [
    "We first look for $\\lambda$ in a logarithmic scale, ranging from $10^{-6}$ to $10^{6}$, comparing the accuracy of the models on the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "74793a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 1e-06, iter: 10000, train_acc: 0.9592254193372426, test_acc: 0.9580662757398064\n",
      "Lambda: 1e-05, iter: 10000, train_acc: 0.9590890495022502, test_acc: 0.9582026455747988\n",
      "Lambda: 0.0001, iter: 10000, train_acc: 0.9592254193372426, test_acc: 0.9579299059048139\n",
      "Lambda: 0.001, iter: 10000, train_acc: 0.9590890495022502, test_acc: 0.9579980908223101\n",
      "Lambda: 0.01, iter: 10000, train_acc: 0.9590890495022502, test_acc: 0.9580662757398064\n",
      "Lambda: 0.1, iter: 10000, train_acc: 0.9592936042547389, test_acc: 0.9583390154097914\n",
      "Lambda: 1, iter: 10000, train_acc: 0.9590890495022502, test_acc: 0.9590890495022502\n",
      "Lambda: 10, iter: 7203, train_acc: 0.9571116868948588, test_acc: 0.9587481249147689\n",
      "Lambda: 100, iter: 960, train_acc: 0.9498159007227601, test_acc: 0.9519978180826402\n",
      "Lambda: 1000, iter: 118, train_acc: 0.9374062457384427, test_acc: 0.9379517250784126\n",
      "Lambda: 10000, iter: 10000, train_acc: 0.5051138688122188, test_acc: 0.5061366425746625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-cf7674abd114>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return -1/(1 + np.exp(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 100000, iter: 321, train_acc: 0.0, test_acc: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-111-9dc4296c0096>:3: RuntimeWarning: overflow encountered in multiply\n",
      "  return  Xs.transpose()*(y*fprime(y*(Xs*w))) + l*w\n",
      "<ipython-input-112-c5cbba0be9fd>:18: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - a*g\n"
     ]
    }
   ],
   "source": [
    "#choose small alpha and a large number of maximum iterations\n",
    "alpha = 0.0001\n",
    "maxit = 10000\n",
    "\n",
    "#The algorithm stops when the norm of the gradient is smaller than grad_threshold\n",
    "grad_threshold = 0.01\n",
    "\n",
    "for lambda_ in [10**i for i in range(-6,6)]:\n",
    "    w,k = lr_sparse_thr(X_train,y_train,alpha,lambda_,grad_threshold,maxit)\n",
    "    train_acc = count_correctly_classified(X_train,y_train,w)/y_train.shape[0]\n",
    "    test_acc = count_correctly_classified(X_test,y_test,w)/y_test.shape[0]\n",
    "    print(\"Lambda: {}, iter: {}, train_acc: {}, test_acc: {}\".format(lambda_, k, train_acc,test_acc))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7fc8de",
   "metadata": {},
   "source": [
    "**Brute-force alpha**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ba368af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.00035, iter: 505, train_acc: 0.9524069275876176, test_acc: 0.9532251465975726\n",
      "Alpha: 0.00036, iter: 491, train_acc: 0.9524069275876176, test_acc: 0.9532251465975726\n",
      "Alpha: 0.00037, iter: 484, train_acc: 0.9524069275876176, test_acc: 0.9532251465975726\n",
      "Alpha: 0.00038, iter: 473, train_acc: 0.9524069275876176, test_acc: 0.9532251465975726\n",
      "Alpha: 0.00039, iter: 443, train_acc: 0.9524069275876176, test_acc: 0.9532251465975726\n",
      "Alpha: 0.0004, iter: 447, train_acc: 0.9524069275876176, test_acc: 0.9532251465975726\n",
      "Alpha: 0.00041000000000000005, iter: 10000, train_acc: 0.9509068594027001, test_acc: 0.9522705577526251\n",
      "Alpha: 0.00042, iter: 10000, train_acc: 0.9464066548479476, test_acc: 0.9474294286103914\n",
      "Alpha: 0.00043000000000000004, iter: 10000, train_acc: 0.9388381290058639, test_acc: 0.9384290195008864\n",
      "Alpha: 0.00044, iter: 10000, train_acc: 0.9366562116459839, test_acc: 0.9363152870585026\n"
     ]
    }
   ],
   "source": [
    "#choose small alpha and a large number of maximum iterations\n",
    "lambda_test = 50\n",
    "maxit = 10000\n",
    "\n",
    "#The algorithm stops when the norm of the gradient is smaller than grad_threshold\n",
    "grad_threshold = 0.01\n",
    "\n",
    "for alpha_ in [0.0004 + i*10**(-5) for i in range(-5,5)]:\n",
    "    w,k = lr_sparse_thr(X_train,y_train,alpha_,lambda_test,grad_threshold,maxit)\n",
    "    train_acc = count_correctly_classified(X_train,y_train,w)/y_train.shape[0]\n",
    "    test_acc = count_correctly_classified(X_test,y_test,w)/y_test.shape[0]\n",
    "    print(\"Alpha: {}, iter: {}, train_acc: {}, test_acc: {}\".format(alpha_, k, train_acc,test_acc))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "09df90ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 0.9590890495022502 0.9579980908223101\n"
     ]
    }
   ],
   "source": [
    "w,k = lr_sparse_thr(X_train,y_train,0.0001,0,0.01,10000)\n",
    "train_acc = count_correctly_classified(X_train,y_train,w)/y_train.shape[0]\n",
    "test_acc = count_correctly_classified(X_test,y_test,w)/y_test.shape[0]\n",
    "\n",
    "print(\"{} {} {}\".format(k,train_acc,test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6552f288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.704488950914988"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s = csr_matrix(X_train)\n",
    "np.linalg.norm(grad_sparse(X_train_s,y_train,w,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6dd4c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w,k = lr_sparse_thr(X_train,y_train,0.00004,8500,0.0001,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9eb2093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8778808127642166"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc = count_correctly_classified(X_train,y_train,w)/y_train.shape[0]\n",
    "test_acc = count_correctly_classified(X_test,y_test,w)/y_test.shape[0]\n",
    "\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768773d7",
   "metadata": {},
   "source": [
    "**Exercise 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e27802ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05843407956007186"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def outlier_removal2 (path):\n",
    "    #loading the data matrix from the file path\n",
    "    [X,y] = load_Xy (path)\n",
    "\n",
    "    #Use the singular value decomposition numpy function to calculate the matrices U, Vt and the vector S of the eigenvalues of X \n",
    "    [U, S, Vt] = np.linalg.svd (X, full_matrices = False)\n",
    "\n",
    "    #Identify the most significant singular values taking the first k greater eigenvalues of S\n",
    "    k = 80\n",
    "\n",
    "    #Create the k-truncated svd matrices\n",
    "    #Consider the matrix U_k taking the first k columns of U\n",
    "    U_k = U [:,0:k]\n",
    "    #Consider the matrix Vt_k taking the first k rows of Vt\n",
    "    Vt_k = Vt [0:k,:]\n",
    "    #Consider the diagonal matrix Sigma_k taking the first k eigenvalues of X\n",
    "    Sigma_k = np.diag (S[0:k])\n",
    "\n",
    "    #Calculating the approximate matrix of X, X_k \n",
    "    X_k = np.dot ( U_k,  np.dot (Sigma_k, Vt_k ))\n",
    "    #X_k = np.dot ( U_k,  Vt_k )\n",
    "\n",
    "    #percentage error of the approximate matrix X_k compared to the real matrix X\n",
    "    p = np.linalg.norm (X-X_k, ord = 'fro') / np.linalg.norm (X, ord = 'fro')\n",
    "    \n",
    "    return [X_k, p]\n",
    "\n",
    "[X_approx, n] = outlier_removal2 ('data2.csv')\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "873fa76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1     8     6 ... 30148 11483   360]\n",
      "Threshold: 10, Removal error: 1.1815185774436276, k: 86\n",
      "[    1     8     6 ... 30148 11483   360]\n",
      "Threshold: 11, Removal error: 1.1815185774436276, k: 86\n",
      "[    1     8     6 ... 30148 11483   360]\n",
      "Threshold: 12, Removal error: 1.1815185774436276, k: 86\n",
      "[30650 30266 30999 ... 10617  5237 11202]\n",
      "Threshold: 13, Removal error: 1.1297019684280325, k: 83\n",
      "[31074 31201 31252 ... 30218 30580 30532]\n",
      "Threshold: 14, Removal error: 1.1284342291163492, k: 77\n",
      "[31074 31252 30736 ...  3989  1602 13502]\n",
      "Threshold: 15, Removal error: 1.1265388774098517, k: 73\n",
      "[31074 31252 30663 ... 30081 31261 25539]\n",
      "Threshold: 16, Removal error: 1.1281262722110248, k: 72\n",
      "[30663 30852 30736 ...  3038 31265 12224]\n",
      "Threshold: 17, Removal error: 1.1320919901163413, k: 67\n",
      "[30111 29478 29768 ... 12216  4192  5311]\n",
      "Threshold: 18, Removal error: 1.1292014676989715, k: 62\n",
      "[29768 29478 30111 ...  2029 30689 10639]\n",
      "Threshold: 19, Removal error: 1.1256832061426556, k: 58\n",
      "[30111 29768  3921 ... 31187 30035 11556]\n",
      "Threshold: 20, Removal error: 1.1264731212038939, k: 55\n"
     ]
    }
   ],
   "source": [
    "X_2,y_2 = load_Xy('data2.csv')\n",
    "def k_truncated_SVD(X,t):\n",
    "    X = X - np.mean(X, axis = 0)\n",
    "    U, S, Vt = np.linalg.svd(X, full_matrices = False)\n",
    "    k = np.where(S>t)[0]\n",
    "    U_k = U[:,k]; Sigma_k = np.diag (S[k]); Vt_k = Vt_k = Vt [k,:]\n",
    "    #print(\"{} {} {}\".format(U_k.shape, Sigma_k.shape, Vt_k.shape))\n",
    "    return len(k), np.dot(U_k, np.dot(Sigma_k, Vt_k))\n",
    "\n",
    "def removal_error(X_1,X,t):\n",
    "    k, X_k = k_truncated_SVD(X,t)\n",
    "    X = X - np.mean(X, axis = 0)\n",
    "    error = np.zeros(X_k.shape[0])\n",
    "    error = np.linalg.norm(X - X_k, axis = 1, ord = 2)\n",
    "    indices = np.argpartition(error, -2000)[-2000:]\n",
    "    sort_ind = indices[np.argsort(-error[indices])]\n",
    "    print(sort_ind)\n",
    "    X_clean = np.delete(X_k, sort_ind, 0)\n",
    "    #print(X_clean.shape, sort_ind.shape)\n",
    "    return k, np.linalg.norm(X_clean - X_1, ord = 'fro')/np.linalg.norm(X_1, ord = 'fro')\n",
    "\n",
    "\n",
    "for t in range(10,21):\n",
    "    k, E = removal_error(X, X_2, t)\n",
    "    print(\"Threshold: {}, Removal error: {}, k: {}\".format(t, E, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "55a807d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_svd (X, k):\n",
    "\n",
    "    #Use the singular value decomposition numpy function to calculate the matrices U, Vt and the vector S of the eigenvalues of X \n",
    "    [U, S, Vt] = np.linalg.svd (X, full_matrices = False)\n",
    "\n",
    "    #Consider the matrix U_k taking the first k columns of U\n",
    "    U_k = U [:,0:k]\n",
    "    #Consider the matrix Vt_k taking the first k rows of Vt\n",
    "    Vt_k = Vt [0:k,:]\n",
    "    #Consider the diagonal matrix Sigma_k taking the first k eigenvalues of X\n",
    "    Sigma_k = np.diag (S[0:k])\n",
    "    #Calculating the k-truncated svd matrices X_k\n",
    "    X_k = np.dot ( U_k,  np.dot (Sigma_k, Vt_k) )\n",
    "\n",
    "    return X_k\n",
    "\n",
    "def detect_outliers (X, k, c):\n",
    "    #create the k-truncated svd of X\n",
    "    X_k = k_svd (X, k)\n",
    "\n",
    "    #calculate the vector of the norm of the difference of the rows between X and X_k\n",
    "    error = np.linalg.norm ( X - X_k, axis = 1 )\n",
    "\n",
    "    #calculate the threshold\n",
    "    #determine the mean of the vector of the error\n",
    "    m = np.mean (error)\n",
    "    #determine the variance of the vector of the error\n",
    "    var = np.var ( error )\n",
    "    #determine the threshold as the sum between the mean and 3 time the variance of the vector of the error\n",
    "    threshold = m + c * var\n",
    "\n",
    "    #determine the outlier as the indices where the error is above the threshold\n",
    "    outlier = np.where (error > threshold)\n",
    "    #redimension the dimension of the array\n",
    "    outlier = np.array(outlier).flatten()\n",
    "\n",
    "    return outlier\n",
    "\n",
    "[X1, y1] = load_Xy ('data.csv')\n",
    "n = X1.shape[0]\n",
    "[X2,y2] = load_Xy ('data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9505bb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.896, 0.15351913084553614, 0.011080049093140597, 2117]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analysis (k, c, n):\n",
    "    outlier = detect_outliers(X2, k, c)\n",
    "    m = len (outlier)\n",
    "\n",
    "    v = np.arange (29332, 31331)\n",
    "    tot = 0\n",
    "    for i in v:\n",
    "        if np.all( np.isin(i, outlier) ):\n",
    "            tot = tot + 1\n",
    "\n",
    "    a = tot / 2000\n",
    "    b = ( m - tot ) / m\n",
    "    d = ( m - tot ) / n\n",
    "\n",
    "    #print ( \"true outliers spotted: {}, fake outliers: {}, modified non-outliers: {}, dimension: {}\".format (a, b, d, len (outlier)) )\n",
    "\n",
    "    return [a, b, d, m]\n",
    "\n",
    "analysis (83, 3, X1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6ac7363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_choice (k, n):\n",
    "    tot_fake = 1\n",
    "    c = 1\n",
    "    j = 1\n",
    "\n",
    "    while j < 20:\n",
    "        [a,  b, d, m] = analysis (k, j, n)\n",
    "\n",
    "        if m >= 2000:\n",
    "            if (b < tot_fake):\n",
    "                c = j\n",
    "                tot_fake = b\n",
    "        \n",
    "        j = j+1\n",
    "    \n",
    "    return c-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "88a352a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-96363d85fc03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;36m86\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_choice\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0moutlier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_outliers\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mX_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk_svd\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-127-599f886c970e>\u001b[0m in \u001b[0;36mc_choice\u001b[1;34m(k, n)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalysis\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-7bbc81245bd3>\u001b[0m in \u001b[0;36manalysis\u001b[1;34m(k, c, n)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutlier\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mtot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtot\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36misin\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ruben\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36misin\u001b[1;34m(element, test_elements, assume_unique, invert)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \"\"\"\n\u001b[0;32m    709\u001b[0m     \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m     return in1d(element, test_elements, assume_unique=assume_unique,\n\u001b[0m\u001b[0;32m    711\u001b[0m                 invert=invert).reshape(element.shape)\n\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36min1d\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ruben\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36min1d\u001b[1;34m(ar1, ar2, assume_unique, invert)\u001b[0m\n\u001b[0;32m    587\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[0mar1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrev_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m         \u001b[0mar2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mar2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ruben\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ruben\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k in [ 86-i for i in range (1,20) ]:\n",
    "    c = c_choice (k, n)\n",
    "    outlier = detect_outliers (X2, k, c)\n",
    "\n",
    "    X_k = k_svd (X2, k)\n",
    "\n",
    "    error_1 = np.linalg.norm ( X1 - X_k[0:n, :], ord = 'fro' ) / np.linalg.norm ( X1, ord = 'fro' )\n",
    "\n",
    "    print(\"k: {}, error_1: {}, c: {}, dimension: {}\".format(k, error_1, c, len (outlier) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f61c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the value k = 83 and c = 3 are considered to us valuable\n",
    "def outlier_removal (X2, y2, k, c):\n",
    "    outlier = detect_outliers (X2, k, c)\n",
    "    X_k = k_svd (X2, k)\n",
    "\n",
    "    X = np.delete (X_k, outlier, axis = 0)\n",
    "    y = np.delete (y2, outlier, axis = 0)\n",
    "\n",
    "    return X,y,len(outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec0db7d",
   "metadata": {},
   "source": [
    "This function returns the percentage of correctly classified points in the test sets after training. It does so for the original dataset $X_1,y_1$ and the dataset $X_2, y_2$ after removing its outliers using the function outlier_removal. It also returns the number of removed outliers from $X_2,y_2$.\n",
    "\n",
    "The inputs are $X_1,y_1$, $X_2,y_2$, a list of hyperparameters for logistic regression and values for the truncation of $X_2,y_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ecafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for X1: 95.84753852447838%, and for X2 after removal of 2117 outliers: 95.31078860898138%\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "hyper = [0.0001, 0.3, 0.01, 4000]\n",
    "\n",
    "#values for the truncation of X2,y2\n",
    "outlier_coeff = [83, 3]\n",
    "\n",
    "#comparison function\n",
    "def w_test_comparison(X1,y1,X2,y2,hyper, outlier_coeff):\n",
    "    #unpacks hyperparameters and values for the k-truncation\n",
    "    a,l,thr,N = hyper\n",
    "    k,c = outlier_coeff\n",
    "\n",
    "    #makes a random 50/50 split of our original data set\n",
    "    X_train, y_train, X_test, y_test = train_test_split(X1,y1,0.5)\n",
    "\n",
    "    #removes outliers from the dataset with fake datapoints, and makes a random 50/50 split\n",
    "    A,b,m = outlier_removal(X2,y2, k,c)\n",
    "    A_train, b_train, A_test, b_test = train_test_split(A,b,0.5)\n",
    "\n",
    "    #runs logistic regression with sparse linear algebra for both the original training set and the training set with removed outliers\n",
    "    w = lr_sparse_thr(X_train, y_train, a,l,thr, N); w_A = lr_sparse_thr(A_train, b_train, a,l,thr, N)\n",
    "\n",
    "    #Calculates the percentage of correctly classified points after calculating the optimal weight vectors in logistic regression.\n",
    "    acc = count_correctly_classified(X_test, y_test, w[0])/y_test.shape[0]*100\n",
    "    acc_A = count_correctly_classified(A_test, b_test, w_A[0])/b_test.shape[0]*100\n",
    "    return m, acc, acc_A\n",
    "m, acc, acc_A = w_test_comparison(X1,y1,X2,y2,hyper, outlier_coeff)\n",
    "print(\"Test accuracy for X1: {}%, and for X2 after removal of {} outliers: {}%\". format(acc, m, acc_A))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
