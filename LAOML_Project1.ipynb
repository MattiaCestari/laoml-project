{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c34894b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "import scipy.sparse as scs\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ac712",
   "metadata": {},
   "source": [
    "**Exercise 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "52c0252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Xy(path):\n",
    "\n",
    "    #Use built-in pandas function to read csv file into a dataframe\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    #Convert the dataframe to a numpy array\n",
    "    data_array = df.to_numpy()\n",
    "\n",
    "    #Separate the features X from the labels y\n",
    "    X,y = data_array[:,:-1], data_array[:,-1]\n",
    "\n",
    "    #Replace the 0 label with -1\n",
    "    y[y==0] = -1 \n",
    "\n",
    "    #return X and y\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "248b9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function and load the data\n",
    "X,y = load_Xy('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "60bf43c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14632"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count how many malicious apps there are\n",
    "# Filter the y array only where y == -1, then get its dimensions, and then its length\n",
    "y[ y==-1 ].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5b12da99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero entries: 277180\n",
      "Total entries: 2522552\n",
      "Ratio: 0.10988078739308446\n"
     ]
    }
   ],
   "source": [
    "#Let's count how many non-zero entries are in X\n",
    "non_zero_entries = X[X!=0].shape[0]\n",
    "\n",
    "#Total elements in X\n",
    "tot_entries = X.shape[0]*X.shape[1]\n",
    "\n",
    "print(\"Non-zero entries: {}\".format(non_zero_entries))\n",
    "print(\"Total entries: {}\".format(tot_entries))\n",
    "print(\"Ratio: {}\".format(non_zero_entries/tot_entries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e843e",
   "metadata": {},
   "source": [
    "Only the $10\\%$ of the elements of the matrix $X$ are non-zero, thus $X$ has a sparsity of $90\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "826da837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd438eba",
   "metadata": {},
   "source": [
    "**Exercise 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633f57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bd01f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_random(X,y,r):    \n",
    "    if not (r > 0 and r < 1):\n",
    "        print(\"r must be between 0 and 1\")\n",
    "        return\n",
    "\n",
    "    train_size = int(y.size*r)\n",
    "    index = range(y.size)\n",
    "    index_train = random.sample(index, train_size)\n",
    "    index_train.sort()\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    for i in range(y.size):\n",
    "        if i in index_train:\n",
    "            y_train.append(y[i])\n",
    "            X_train.append(X[i,:])\n",
    "        else:\n",
    "            y_test.append(y[i])\n",
    "            X_test.append(X[i,:])\n",
    "    X_train = np.vstack(X_train)\n",
    "    X_test = np.vstack(X_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_random(X,y, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "47c0207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X,y,r):\n",
    "\n",
    "    #Check that r is between 0 and 1\n",
    "    if not ( r > 0 and r < 1):\n",
    "        print(\"r must be between 0 and 1\")\n",
    "        return\n",
    "        \n",
    "    #get number of rows\n",
    "    rows = X.shape[0]\n",
    "\n",
    "    #create a indices array for the rows\n",
    "    indices = [i for i in range(rows)]\n",
    "\n",
    "    #shuffle the array randomly\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    #Now that the indices are randomized, we can split in train and test\n",
    "    train_indices = indices[:int(rows*r)]\n",
    "    test_indices = indices[int(rows*r):]\n",
    "\n",
    "    X_train = X[train_indices,:]\n",
    "    X_test = X[test_indices,:]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a05cf253",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_test_split(X,y,0.8)\n",
    "X_train1, y_train1, X_test1, y_test1 = train_test_split(X,y,0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e453d3f3",
   "metadata": {},
   "source": [
    "**Exercise 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5965fdc",
   "metadata": {},
   "source": [
    "Let $X \\in \\R^{m \\times n}$ be the data matrix with data columns $\\textbf{x}_i,\\ldots, \\textbf{x}_n, i = 1,\\ldots, n$, vector $\\textbf{y} \\in \\{-1,1\\}^n$, and a weight vector $\\textbf{w} \\in \\R^m$. If the product $y_i \\textbf{x}_i^\\intercal \\textbf{w} > 0$, the point is correctly classified, and if $y_i \\textbf{x}_i^\\intercal \\textbf{w} <0$, the point is misclassified. We count the points $\\textbf{x}_i^\\intercal \\textbf{w} = 0$ as misclassified, as there is no way of confirming its (mis)classification.\n",
    "\n",
    "We define a function $f$ that assigns value $0$ to misclassified points, and $1$ for correctly classified points. Mathematically, this function would be defined as $f: \\R \\rightarrow \\{0,1\\}$:\n",
    "\n",
    "$f(s) \\coloneqq \\frac{1}{2}(\\text{sign}(s) + 1)$,\n",
    "\n",
    "with $f(0) = 0$.\n",
    "\n",
    "The function for the number of correctly classified points $F: \\R^m \\rightarrow \\Z_{\\geq 0}$ is then defined:\n",
    "\n",
    "$F(\\textbf{w}) = \\sum_{i = 1} ^ n f(y_i\\textbf{x}_i^\\intercal \\textbf{w})$.\n",
    "\n",
    "Since Python does not have a sign function, we write a function that fulfills this task. \n",
    "\n",
    "We generate a random weight vector $w \\in \\R^m$ with values $w_j \\in [-10,10), \\forall j = 1,\\ldots,m$, such that the chances of $w_j$ being positive or negative are equal and the chance of $w_j = 0$ is approximately zero, but we include a safety measure to stop the function in the case $\\textbf{w}$ contains at least one $0$ element.\n",
    "\n",
    "The function for the sum of correctly classified points is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a25862dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_classification(X,y,w):\n",
    "    if np.all(w) == False:\n",
    "        print(\"w contains at least one element 0\")\n",
    "        return\n",
    "    I = y*np.dot(X, w) #compute the product y_ix_iw elementwise, I is the indicator vector\n",
    "    C = 0 #starting value of the number of correct points\n",
    "    for i in range(I.size):\n",
    "        if I[i] <= 0: #misclassified\n",
    "            continue\n",
    "        else:   #correctly classified\n",
    "            C += 1\n",
    "    return I,C\n",
    "dim = len(X[0])\n",
    "w = np.random.uniform(-10, 10, (dim)) #random uniform weight vector with equal chances of positive and negative elements\n",
    "I,C = correct_classification(X, y, w) #test with X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cb3c8",
   "metadata": {},
   "source": [
    "**Exercise 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda06b48",
   "metadata": {},
   "source": [
    "The Hinge-Loss function for logistic regression is:\n",
    "\n",
    "$g(s) = \\log{(1+e^{-s})}$.\n",
    "\n",
    "As such, the cost function for logistic regression is defined as:\n",
    "\n",
    "$J(\\textbf{w}) = \\sum_{i=1}^{n} \\log{(1+e^{-y_i \\textbf{x}_i^T \\textbf{w}})} + \\frac{\\lambda}{2} \\lVert \\textbf{w} \\rVert^2$,\n",
    "\n",
    "with gradient:\n",
    "\n",
    "$\\nabla J (\\textbf{w}) = \\sum_{i=1}^{n} -\\frac {e^{-y_i \\textbf{x}_i^T \\textbf{w}}}{1+e^{-y_i \\textbf{x}_i^T \\textbf{w}}}y_i\\boldsymbol{x}_i + \\lambda \\textbf{w}$.\n",
    "\n",
    "This gradient is determined as follows: consider the functions $g(s) = \\log {(1+e^{-s})}$ and $h_i (\\textbf{w}) = y_i \\textbf{x}i^T \\textbf{w}$. The cost function then becomes:\n",
    "\n",
    "$J(\\textbf{w}) = \\sum_{i=1}^{n} g (h_i (\\textbf{w})) + \\frac{\\lambda}{2}\\lVert\\textbf{w} \\rVert^2$.\n",
    "\n",
    "Note that the derivatives of $g$ and $h_i$ are defined as:\n",
    "\n",
    "$g^\\prime (s) = - \\frac {e^{-s}}  {1+e^{-s}} = - \\frac {1} {1+e^{s}}\\quad$ , $\\quad\\nabla h_i (\\textbf{w}) = y_i \\textbf{x}_i$,\n",
    "\n",
    "and trivially: $\\frac{d}{d\\textbf{w}}\\left(\\frac{\\lambda}{2}\\lVert\\textbf{w} \\rVert^2\\right) = \\lambda \\textbf{w}$.\n",
    "\n",
    "Then, due to the chain rule of the Jacobian, the gradient of the cost function becomes:\n",
    "\n",
    "$\\nabla J (\\textbf{w}) = \\sum_{i=1}^{n} g^\\prime (y_i \\textbf{x}_i^T \\textbf{w}) \\cdot \\nabla h_i (\\textbf{w}) + \\lambda \\textbf{w}$, \n",
    "\n",
    "obtaining the formula above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a5cda",
   "metadata": {},
   "source": [
    "**Exercise 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f6ddddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20331.39310018            inf            inf ...     0.\n",
      "     0.             0.        ]\n",
      "[-4.14524370e+02 -1.24657353e+03  7.89022059e+02 -1.98955882e+02\n",
      " -1.66905163e+02 -3.69365732e+03  7.17507353e+02  5.53382353e+01\n",
      "  1.12454121e+03 -1.49483088e+03  4.55882353e-01  2.55183569e+03\n",
      " -3.45857683e+03 -1.00146324e+03 -8.37573529e+02 -6.42573529e+02\n",
      " -2.48823529e+01  2.85844148e+03 -1.32500000e+02  1.16938918e+02\n",
      "  6.45234543e+02  4.99502746e+03 -1.50000000e+02 -2.96766964e+02\n",
      "  3.10070624e+02 -8.43170048e+03  5.28067718e+02 -1.18257353e+03\n",
      " -1.34904412e+02 -1.08365441e+03 -1.53966296e+03 -3.48308824e+02\n",
      " -6.27573529e+02 -6.97000000e+02  2.46786765e+02 -1.26904412e+02\n",
      " -1.45058824e+02 -1.19757353e+03 -1.68000000e+02 -7.37199265e+03\n",
      "  1.92198529e+02 -8.32573529e+02 -1.24532353e+03 -3.62352941e+02\n",
      "  7.98843330e+02 -8.30073529e+02 -2.85000000e+02 -8.01617647e+01\n",
      "  1.47451664e+02 -6.33823529e+02  7.68195624e+02 -5.08823529e+02\n",
      " -1.23455882e+02  1.79909364e+03  5.87867647e+02 -1.50301471e+02\n",
      " -1.41750000e+02  2.10262981e+03 -4.61250000e+01 -3.48750000e+02\n",
      " -5.08823529e+02  4.04993003e+02  9.60430814e+02 -3.48746324e+03\n",
      " -3.52852941e+02  2.61288775e+03 -4.45862464e+03  1.58089511e+03\n",
      " -8.22110294e+02 -1.69743382e+03 -3.05404412e+02 -1.42500000e+02\n",
      " -6.09919118e+02 -2.29198825e+03 -2.21750000e+02 -1.35758538e+02\n",
      " -8.29117647e+01  2.27588707e+02 -1.37580882e+02 -1.21857353e+03\n",
      " -1.24782353e+03 -7.74869312e+03 -8.70000000e+02 -8.43823529e+02\n",
      "  7.31756602e+02  7.36985294e+01]\n",
      "Running time:  14.428948163986206  seconds\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore') #to ignore the RunTimeWarning\n",
    "def logistic_regression(X,y,a,l,K):\n",
    "    dim = len(X[0])\n",
    "    w = np.zeros(dim)\n",
    "    loss = np.zeros (len(y))\n",
    "    for i in range(K):\n",
    "        I = y*np.dot(X,w)\n",
    "        loss[i] = np.sum( np.log (1+ np.exp (-I) ) ) + l/2*np.dot (w.T,w)\n",
    "        I = (1 + np.exp(I))\n",
    "        I = - 1 / I\n",
    "        I = I * y\n",
    "        J = np.dot(X.T,I) + l*w\n",
    "        w = w - a*J\n",
    "    print (loss)\n",
    "    return w\n",
    "\n",
    "t0 = time.time()\n",
    "w = logistic_regression(X,y,0.8,0.5,1000)\n",
    "t1 = time.time()\n",
    "print(w)\n",
    "print('Running time: ', str(t1-t0), ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b0c16a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20331.39310018            inf            inf ...     0.\n",
      "     0.             0.        ]\n",
      "[-4.14524370e+02 -1.24657353e+03  7.89022059e+02 -1.98955882e+02\n",
      " -1.66905163e+02 -3.69365732e+03  7.17507353e+02  5.53382353e+01\n",
      "  1.12454121e+03 -1.49483088e+03  4.55882353e-01  2.55183569e+03\n",
      " -3.45857683e+03 -1.00146324e+03 -8.37573529e+02 -6.42573529e+02\n",
      " -2.48823529e+01  2.85844148e+03 -1.32500000e+02  1.16938918e+02\n",
      "  6.45234543e+02  4.99502746e+03 -1.50000000e+02 -2.96766964e+02\n",
      "  3.10070624e+02 -8.43170048e+03  5.28067718e+02 -1.18257353e+03\n",
      " -1.34904412e+02 -1.08365441e+03 -1.53966296e+03 -3.48308824e+02\n",
      " -6.27573529e+02 -6.97000000e+02  2.46786765e+02 -1.26904412e+02\n",
      " -1.45058824e+02 -1.19757353e+03 -1.68000000e+02 -7.37199265e+03\n",
      "  1.92198529e+02 -8.32573529e+02 -1.24532353e+03 -3.62352941e+02\n",
      "  7.98843330e+02 -8.30073529e+02 -2.85000000e+02 -8.01617647e+01\n",
      "  1.47451664e+02 -6.33823529e+02  7.68195624e+02 -5.08823529e+02\n",
      " -1.23455882e+02  1.79909364e+03  5.87867647e+02 -1.50301471e+02\n",
      " -1.41750000e+02  2.10262981e+03 -4.61250000e+01 -3.48750000e+02\n",
      " -5.08823529e+02  4.04993003e+02  9.60430814e+02 -3.48746324e+03\n",
      " -3.52852941e+02  2.61288775e+03 -4.45862464e+03  1.58089511e+03\n",
      " -8.22110294e+02 -1.69743382e+03 -3.05404412e+02 -1.42500000e+02\n",
      " -6.09919118e+02 -2.29198825e+03 -2.21750000e+02 -1.35758538e+02\n",
      " -8.29117647e+01  2.27588707e+02 -1.37580882e+02 -1.21857353e+03\n",
      " -1.24782353e+03 -7.74869312e+03 -8.70000000e+02 -8.43823529e+02\n",
      "  7.31756602e+02  7.36985294e+01]\n",
      "Running time:  3.6182966232299805  seconds\n"
     ]
    }
   ],
   "source": [
    "#now using sparse linear algebra\n",
    "def log_reg_sparse(X,y,a,l,K):\n",
    "    dim = len(X[0])\n",
    "    X_sparse = scs.csr_matrix(X) #changes X from dense to CSR format\n",
    "    w = np.zeros(dim)\n",
    "    loss = np.zeros(len(y))\n",
    "    for i in range(K):\n",
    "        I = y*(X_sparse@w)\n",
    "        loss[i] = np.sum( np.log (1+ np.exp (-I) ) ) + l/2*np.dot (w.T,w)\n",
    "        J = -1/(np.exp(I)+1)*y\n",
    "        J = J@X_sparse + l*w\n",
    "        w = w - a*J\n",
    "    print(loss)\n",
    "    return w\n",
    "\n",
    "t0_s = time.time()\n",
    "w = log_reg_sparse(X,y,0.8,0.5,1000)\n",
    "t1_s = time.time()\n",
    "print(w)\n",
    "print('Running time: ', str(t1_s-t0_s), ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2eac1c",
   "metadata": {},
   "source": [
    "**Exercise 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891a70a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b03fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
